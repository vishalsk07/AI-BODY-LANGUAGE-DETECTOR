{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Import depencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic.HAND_CONNECTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Make detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor feed\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Make detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        image.flags.writeable=True\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=1,circle_radius=1))\n",
    "\n",
    "        # 2. Left hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 3. Right hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 4. Pose detection\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        cv2.imshow(\"Holistic model detection\",image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Capture landmarks and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coords = len(results.pose_landmarks.landmark) + len(results.face_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1,num_coords+1):\n",
    "    landmarks+=[f\"x{val}, y{val}, z{val}, v{val}\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "os.makedirs('data',exist_ok=True)\n",
    "with open('data/coords.csv',mode=\"w\",newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f,delimiter=\",\",quotechar=\"'\",quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"Happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor feed\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Make detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        image.flags.writeable=True\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=1,circle_radius=1))\n",
    "\n",
    "        # 2. Left hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 3. Right hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 4. Pose detection\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x,landmark.y,landmark.z,landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # Extract face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x,landmark.y,landmark.z,landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            # Concat rows\n",
    "            row = pose_row+face_row\n",
    "\n",
    "            # Insert row\n",
    "            row.insert(0,class_name)\n",
    "\n",
    "            # Export to csv\n",
    "            with open('data/coords.csv',mode='a',newline='') as f:\n",
    "                csv_writer = csv.writer(f,delimiter=\",\",quotechar=\"'\",quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        cv2.imshow(\"Holistic model detection\",image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Train custom model using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read in collected data and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Victorius</th>\n",
       "      <th>0.4979402422904968</th>\n",
       "      <th>0.4681210517883301</th>\n",
       "      <th>-0.8602403402328491</th>\n",
       "      <th>0.9999120235443115</th>\n",
       "      <th>0.526459813117981</th>\n",
       "      <th>0.4148291349411011</th>\n",
       "      <th>-0.8045721054077148</th>\n",
       "      <th>0.9998873472213745</th>\n",
       "      <th>0.5431056022644043</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.002525888616219163</th>\n",
       "      <th>0.0.465</th>\n",
       "      <th>0.5705472826957703</th>\n",
       "      <th>0.41139546036720276</th>\n",
       "      <th>0.012656714767217636</th>\n",
       "      <th>0.0.466</th>\n",
       "      <th>0.5750494003295898</th>\n",
       "      <th>0.4069294035434723</th>\n",
       "      <th>0.0130736930295825</th>\n",
       "      <th>0.0.467</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Victorius</td>\n",
       "      <td>0.524360</td>\n",
       "      <td>0.502162</td>\n",
       "      <td>-0.562991</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.556463</td>\n",
       "      <td>0.454222</td>\n",
       "      <td>-0.486707</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.576274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580852</td>\n",
       "      <td>0.447459</td>\n",
       "      <td>-0.008492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585508</td>\n",
       "      <td>0.442914</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>Happy</td>\n",
       "      <td>0.382764</td>\n",
       "      <td>0.522209</td>\n",
       "      <td>-1.116260</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.431091</td>\n",
       "      <td>-1.048760</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.452885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477971</td>\n",
       "      <td>0.420898</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484933</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Happy</td>\n",
       "      <td>0.903546</td>\n",
       "      <td>0.586754</td>\n",
       "      <td>-1.226071</td>\n",
       "      <td>0.998469</td>\n",
       "      <td>0.930665</td>\n",
       "      <td>0.515564</td>\n",
       "      <td>-1.141603</td>\n",
       "      <td>0.996286</td>\n",
       "      <td>0.945367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.026155</td>\n",
       "      <td>0.548450</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.036082</td>\n",
       "      <td>0.543244</td>\n",
       "      <td>0.038877</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Sad</td>\n",
       "      <td>0.392915</td>\n",
       "      <td>0.676347</td>\n",
       "      <td>-0.610584</td>\n",
       "      <td>0.998945</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.617388</td>\n",
       "      <td>-0.609081</td>\n",
       "      <td>0.997950</td>\n",
       "      <td>0.417376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.439795</td>\n",
       "      <td>0.591383</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.586325</td>\n",
       "      <td>-0.010181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Victorius</td>\n",
       "      <td>0.549120</td>\n",
       "      <td>0.470696</td>\n",
       "      <td>-0.853573</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.588575</td>\n",
       "      <td>0.399984</td>\n",
       "      <td>-0.779457</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0.613439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628756</td>\n",
       "      <td>0.377649</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633639</td>\n",
       "      <td>0.372683</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Victorius  0.4979402422904968  0.4681210517883301  -0.8602403402328491  \\\n",
       "139  Victorius            0.524360            0.502162            -0.562991   \n",
       "930      Happy            0.382764            0.522209            -1.116260   \n",
       "996      Happy            0.903546            0.586754            -1.226071   \n",
       "454        Sad            0.392915            0.676347            -0.610584   \n",
       "225  Victorius            0.549120            0.470696            -0.853573   \n",
       "\n",
       "     0.9999120235443115  0.526459813117981  0.4148291349411011  \\\n",
       "139            0.999988           0.556463            0.454222   \n",
       "930            0.999971           0.425730            0.431091   \n",
       "996            0.998469           0.930665            0.515564   \n",
       "454            0.998945           0.404959            0.617388   \n",
       "225            0.999891           0.588575            0.399984   \n",
       "\n",
       "     -0.8045721054077148  0.9998873472213745  0.5431056022644043  ...  \\\n",
       "139            -0.486707            0.999985            0.576274  ...   \n",
       "930            -1.048760            0.999918            0.452885  ...   \n",
       "996            -1.141603            0.996286            0.945367  ...   \n",
       "454            -0.609081            0.997950            0.417376  ...   \n",
       "225            -0.779457            0.999805            0.613439  ...   \n",
       "\n",
       "     -0.002525888616219163  0.0.465  0.5705472826957703  0.41139546036720276  \\\n",
       "139              -0.005465      0.0            0.580852             0.447459   \n",
       "930              -0.011770      0.0            0.477971             0.420898   \n",
       "996              -0.001712      0.0            1.026155             0.548450   \n",
       "454              -0.011673      0.0            0.439795             0.591383   \n",
       "225              -0.009587      0.0            0.628756             0.377649   \n",
       "\n",
       "     0.012656714767217636  0.0.466  0.5750494003295898  0.4069294035434723  \\\n",
       "139             -0.008492      0.0            0.585508            0.442914   \n",
       "930              0.012765      0.0            0.484933            0.410403   \n",
       "996              0.036842      0.0            1.036082            0.543244   \n",
       "454             -0.009302      0.0            0.442105            0.586325   \n",
       "225              0.011414      0.0            0.633639            0.372683   \n",
       "\n",
       "     0.0130736930295825  0.0.467  \n",
       "139           -0.009618      0.0  \n",
       "930            0.013926      0.0  \n",
       "996            0.038877      0.0  \n",
       "454           -0.010181      0.0  \n",
       "225            0.011802      0.0  \n",
       "\n",
       "[5 rows x 2005 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/coords.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,1:]\n",
    "y=df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train machine learning classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"lr\":make_pipeline(StandardScaler(),LogisticRegression()),\n",
    "    \"rc\":make_pipeline(StandardScaler(),RidgeClassifier()),\n",
    "    \"rf\":make_pipeline(StandardScaler(),RandomForestClassifier()),\n",
    "    \"gb\":make_pipeline(StandardScaler(),GradientBoostingClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipelines.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo,pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train,y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('ridgeclassifier', RidgeClassifier())]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier', RandomForestClassifier())]),\n",
       " 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sad', 'Sad', 'Happy', 'Happy', 'Victorius', 'Sad', 'Victorius',\n",
       "       'Sad', 'Victorius', 'Sad', 'Happy', 'Happy', 'Victorius', 'Sad',\n",
       "       'Happy', 'Happy', 'Sad', 'Sad', 'Happy', 'Happy', 'Victorius',\n",
       "       'Happy', 'Sad', 'Happy', 'Sad', 'Victorius', 'Victorius',\n",
       "       'Victorius', 'Victorius', 'Victorius', 'Victorius', 'Happy', 'Sad',\n",
       "       'Sad', 'Victorius', 'Happy', 'Sad', 'Victorius', 'Sad',\n",
       "       'Victorius', 'Happy', 'Happy', 'Sad', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Victorius', 'Sad', 'Happy', 'Happy', 'Happy',\n",
       "       'Victorius', 'Happy', 'Happy', 'Sad', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Sad', 'Happy', 'Happy', 'Happy', 'Sad', 'Happy',\n",
       "       'Victorius', 'Sad', 'Sad', 'Happy', 'Sad', 'Happy', 'Sad', 'Happy',\n",
       "       'Sad', 'Sad', 'Sad', 'Victorius', 'Happy', 'Victorius', 'Happy',\n",
       "       'Sad', 'Victorius', 'Happy', 'Sad', 'Victorius', 'Sad',\n",
       "       'Victorius', 'Sad', 'Happy', 'Victorius', 'Victorius', 'Victorius',\n",
       "       'Sad', 'Victorius', 'Victorius', 'Sad', 'Sad', 'Happy', 'Happy',\n",
       "       'Victorius', 'Happy', 'Victorius', 'Victorius', 'Sad', 'Sad',\n",
       "       'Victorius', 'Victorius', 'Happy', 'Victorius', 'Victorius', 'Sad',\n",
       "       'Happy', 'Happy', 'Victorius', 'Victorius', 'Sad', 'Happy',\n",
       "       'Happy', 'Sad', 'Sad', 'Sad', 'Happy', 'Sad', 'Sad', 'Sad',\n",
       "       'Victorius', 'Sad', 'Victorius', 'Victorius', 'Victorius', 'Sad',\n",
       "       'Victorius', 'Victorius', 'Sad', 'Victorius', 'Happy', 'Victorius',\n",
       "       'Happy', 'Happy', 'Victorius', 'Happy', 'Victorius', 'Victorius',\n",
       "       'Victorius', 'Victorius', 'Sad', 'Victorius', 'Sad', 'Victorius',\n",
       "       'Sad', 'Happy', 'Sad', 'Victorius', 'Victorius', 'Happy',\n",
       "       'Victorius', 'Happy', 'Sad', 'Sad', 'Happy', 'Sad', 'Sad', 'Sad',\n",
       "       'Victorius', 'Sad', 'Victorius', 'Happy', 'Happy', 'Victorius',\n",
       "       'Sad', 'Happy', 'Sad', 'Victorius', 'Sad', 'Sad', 'Sad', 'Sad',\n",
       "       'Victorius', 'Happy', 'Sad', 'Victorius', 'Sad', 'Sad', 'Happy',\n",
       "       'Happy', 'Happy', 'Victorius', 'Victorius', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Sad', 'Victorius', 'Victorius', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Victorius', 'Victorius', 'Victorius', 'Sad', 'Sad',\n",
       "       'Sad', 'Happy', 'Sad', 'Sad', 'Happy', 'Sad', 'Sad', 'Happy',\n",
       "       'Victorius', 'Sad', 'Happy', 'Sad', 'Sad', 'Happy', 'Victorius',\n",
       "       'Sad', 'Happy', 'Victorius', 'Sad', 'Happy', 'Sad', 'Happy', 'Sad',\n",
       "       'Victorius', 'Happy', 'Sad', 'Happy', 'Sad', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Sad', 'Sad', 'Victorius', 'Happy', 'Victorius',\n",
       "       'Sad', 'Happy', 'Happy', 'Sad', 'Happy', 'Sad', 'Victorius', 'Sad',\n",
       "       'Happy', 'Victorius', 'Victorius', 'Sad', 'Victorius', 'Happy',\n",
       "       'Victorius', 'Sad', 'Sad', 'Victorius', 'Happy', 'Victorius',\n",
       "       'Victorius', 'Happy', 'Victorius', 'Sad', 'Sad', 'Victorius',\n",
       "       'Victorius', 'Victorius', 'Victorius', 'Sad', 'Sad', 'Victorius',\n",
       "       'Sad', 'Sad', 'Victorius', 'Sad', 'Happy', 'Victorius', 'Sad',\n",
       "       'Happy', 'Victorius', 'Victorius', 'Happy', 'Sad', 'Happy', 'Sad',\n",
       "       'Happy', 'Victorius', 'Sad', 'Sad', 'Victorius', 'Victorius',\n",
       "       'Victorius', 'Happy', 'Victorius', 'Victorius', 'Sad', 'Sad',\n",
       "       'Sad', 'Victorius', 'Victorius', 'Sad', 'Victorius', 'Sad', 'Sad',\n",
       "       'Sad', 'Sad', 'Sad', 'Sad', 'Victorius', 'Victorius', 'Sad',\n",
       "       'Victorius', 'Victorius', 'Happy', 'Sad', 'Sad', 'Happy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['lr'].predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate and serialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 1.0\n",
      "rf 0.9968253968253968\n",
      "gb 0.9968253968253968\n"
     ]
    }
   ],
   "source": [
    "for algo,model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo,accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((fit_models['rf'].predict(X_test)==y_test)==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('model',exist_ok=True)\n",
    "with open('model/pose_detection.pkl','wb') as f:\n",
    "    pickle.dump(fit_models['rf'],f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Make detections with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model/pose_detection.pkl','rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow for detections\n",
    "1. Detect landmarks\n",
    "2. Predict pose (class) based on landmark coordinates\n",
    "3. Render landmarks and body language pose using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy [0.83 0.11 0.06]\n",
      "Happy [0.87 0.1  0.03]\n",
      "Happy [0.88 0.09 0.03]\n",
      "Happy [0.86 0.11 0.03]\n",
      "Happy [0.85 0.12 0.03]\n",
      "Happy [0.85 0.11 0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.87 0.09 0.04]\n",
      "Happy [0.88 0.08 0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.89 0.08 0.03]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.87 0.09 0.04]\n",
      "Happy [0.88 0.08 0.04]\n",
      "Happy [0.87 0.09 0.04]\n",
      "Happy [0.88 0.08 0.04]\n",
      "Happy [0.88 0.08 0.04]\n",
      "Happy [0.87 0.08 0.05]\n",
      "Happy [0.87 0.08 0.05]\n",
      "Happy [0.88 0.08 0.04]\n",
      "Happy [0.86 0.09 0.05]\n",
      "Happy [0.86 0.09 0.05]\n",
      "Happy [0.87 0.09 0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.85 0.12 0.03]\n",
      "Happy [0.85 0.12 0.03]\n",
      "Happy [0.86 0.11 0.03]\n",
      "Happy [0.85 0.11 0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.85 0.11 0.04]\n",
      "Happy [0.85 0.11 0.04]\n",
      "Happy [0.84 0.11 0.05]\n",
      "Happy [0.84 0.12 0.04]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.84 0.1  0.06]\n",
      "Happy [0.83 0.11 0.06]\n",
      "Happy [0.84 0.1  0.06]\n",
      "Happy [0.81 0.11 0.08]\n",
      "Happy [0.87 0.08 0.05]\n",
      "Happy [0.86 0.08 0.06]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.85 0.1  0.05]\n",
      "Happy [0.85 0.1  0.05]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.87 0.08 0.05]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.88 0.07 0.05]\n",
      "Happy [0.8 0.1 0.1]\n",
      "Happy [0.8  0.11 0.09]\n",
      "Happy [0.81 0.1  0.09]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.82 0.1  0.08]\n",
      "Happy [0.82 0.12 0.06]\n",
      "Happy [0.83 0.11 0.06]\n",
      "Happy [0.83 0.1  0.07]\n",
      "Happy [0.82 0.12 0.06]\n",
      "Happy [0.82 0.12 0.06]\n",
      "Happy [0.82 0.11 0.07]\n",
      "Happy [0.81 0.1  0.09]\n",
      "Happy [0.79 0.09 0.12]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.81 0.08 0.11]\n",
      "Happy [0.81 0.08 0.11]\n",
      "Happy [0.8  0.11 0.09]\n",
      "Happy [0.82 0.11 0.07]\n",
      "Happy [0.82 0.11 0.07]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.85 0.1  0.05]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.84 0.09 0.07]\n",
      "Happy [0.86 0.08 0.06]\n",
      "Happy [0.87 0.08 0.05]\n",
      "Happy [0.8  0.11 0.09]\n",
      "Happy [0.83 0.09 0.08]\n",
      "Happy [0.79 0.12 0.09]\n",
      "Happy [0.85 0.1  0.05]\n",
      "Happy [0.87 0.07 0.06]\n",
      "Happy [0.83 0.11 0.06]\n",
      "Happy [0.83 0.11 0.06]\n",
      "Happy [0.84 0.09 0.07]\n",
      "Happy [0.83 0.1  0.07]\n",
      "Happy [0.84 0.1  0.06]\n",
      "Happy [0.8 0.1 0.1]\n",
      "Happy [0.81 0.09 0.1 ]\n",
      "Happy [0.82 0.08 0.1 ]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.86 0.08 0.06]\n",
      "Happy [0.86 0.07 0.07]\n",
      "Happy [0.79 0.09 0.12]\n",
      "Happy [0.8  0.09 0.11]\n",
      "Happy [0.82 0.07 0.11]\n",
      "Happy [0.79 0.09 0.12]\n",
      "Happy [0.8  0.08 0.12]\n",
      "Happy [0.8  0.08 0.12]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.81 0.1  0.09]\n",
      "Happy [0.81 0.11 0.08]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.86 0.08 0.06]\n",
      "Happy [0.86 0.07 0.07]\n",
      "Happy [0.83 0.07 0.1 ]\n",
      "Happy [0.79 0.11 0.1 ]\n",
      "Happy [0.8  0.09 0.11]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.84 0.09 0.07]\n",
      "Happy [0.84 0.09 0.07]\n",
      "Happy [0.8  0.08 0.12]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.78 0.11 0.11]\n",
      "Happy [0.84 0.1  0.06]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.85 0.09 0.06]\n",
      "Happy [0.79 0.1  0.11]\n",
      "Happy [0.78 0.12 0.1 ]\n",
      "Happy [0.79 0.11 0.1 ]\n",
      "Happy [0.79 0.1  0.11]\n",
      "Happy [0.86 0.1  0.04]\n",
      "Happy [0.79 0.1  0.11]\n",
      "Happy [0.79 0.11 0.1 ]\n",
      "Happy [0.82 0.11 0.07]\n",
      "Happy [0.83 0.12 0.05]\n",
      "Happy [0.79 0.12 0.09]\n",
      "Happy [0.86 0.11 0.03]\n",
      "Happy [0.84 0.11 0.05]\n",
      "Happy [0.88 0.06 0.06]\n",
      "Happy [0.87 0.03 0.1 ]\n",
      "Happy [0.84 0.05 0.11]\n",
      "Happy [0.76 0.09 0.15]\n",
      "Happy [0.51 0.35 0.14]\n",
      "Sad [0.41 0.43 0.16]\n",
      "Sad [0.35 0.47 0.18]\n",
      "Sad [0.29 0.51 0.2 ]\n",
      "Sad [0.28 0.47 0.25]\n",
      "Sad [0.24 0.46 0.3 ]\n",
      "Victorius [0.21 0.39 0.4 ]\n",
      "Victorius [0.21 0.39 0.4 ]\n",
      "Sad [0.19 0.41 0.4 ]\n",
      "Sad [0.18 0.42 0.4 ]\n",
      "Sad [0.19 0.42 0.39]\n",
      "Sad [0.19 0.44 0.37]\n",
      "Happy [0.34 0.34 0.32]\n",
      "Sad [0.37 0.38 0.25]\n",
      "Sad [0.13 0.67 0.2 ]\n",
      "Sad [0.05 0.73 0.22]\n",
      "Sad [0.04 0.81 0.15]\n",
      "Sad [0.03 0.84 0.13]\n",
      "Sad [0.03 0.87 0.1 ]\n",
      "Sad [0.02 0.88 0.1 ]\n",
      "Sad [0.02 0.88 0.1 ]\n",
      "Sad [0.02 0.88 0.1 ]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.03 0.89 0.08]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.01 0.96 0.03]\n",
      "Sad [0.01 0.97 0.02]\n",
      "Sad [0.01 0.96 0.03]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.01 0.96 0.03]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.01 0.96 0.03]\n",
      "Sad [0.02 0.94 0.04]\n",
      "Sad [0.01 0.97 0.02]\n",
      "Sad [0.02 0.95 0.03]\n",
      "Sad [0.02 0.92 0.06]\n",
      "Sad [0.04 0.93 0.03]\n",
      "Sad [0.44 0.48 0.08]\n",
      "Happy [0.56 0.32 0.12]\n",
      "Happy [0.84 0.02 0.14]\n",
      "Happy [0.65 0.14 0.21]\n",
      "Happy [0.44 0.32 0.24]\n",
      "Happy [0.43 0.33 0.24]\n",
      "Happy [0.41 0.32 0.27]\n",
      "Happy [0.39 0.32 0.29]\n",
      "Happy [0.35 0.32 0.33]\n",
      "Happy [0.36 0.3  0.34]\n",
      "Happy [0.39 0.32 0.29]\n",
      "Happy [0.4  0.32 0.28]\n",
      "Happy [0.37 0.31 0.32]\n",
      "Happy [0.43 0.32 0.25]\n",
      "Happy [0.4  0.33 0.27]\n",
      "Sad [0.32 0.35 0.33]\n",
      "Happy [0.59 0.23 0.18]\n",
      "Happy [0.45 0.38 0.17]\n",
      "Sad [0.15 0.72 0.13]\n",
      "Sad [0.08 0.74 0.18]\n",
      "Sad [0.09 0.75 0.16]\n",
      "Sad [0.08 0.75 0.17]\n",
      "Sad [0.08 0.74 0.18]\n",
      "Sad [0.07 0.67 0.26]\n",
      "Sad [0.07 0.67 0.26]\n",
      "Sad [0.07 0.68 0.25]\n",
      "Sad [0.06 0.74 0.2 ]\n",
      "Sad [0.05 0.78 0.17]\n",
      "Sad [0.07 0.72 0.21]\n",
      "Sad [0.07 0.72 0.21]\n",
      "Sad [0.07 0.72 0.21]\n",
      "Sad [0.07 0.74 0.19]\n",
      "Sad [0.31 0.44 0.25]\n",
      "Happy [0.59 0.2  0.21]\n",
      "Happy [0.67 0.04 0.29]\n",
      "Happy [0.58 0.02 0.4 ]\n",
      "Victorius [0.27 0.3  0.43]\n",
      "Victorius [0.28 0.29 0.43]\n",
      "Victorius [0.29 0.27 0.44]\n",
      "Victorius [0.29 0.27 0.44]\n",
      "Victorius [0.27 0.29 0.44]\n",
      "Victorius [0.28 0.29 0.43]\n",
      "Victorius [0.3  0.29 0.41]\n",
      "Victorius [0.31 0.29 0.4 ]\n",
      "Victorius [0.32 0.3  0.38]\n",
      "Happy [0.4 0.3 0.3]\n",
      "Happy [0.37 0.31 0.32]\n",
      "Happy [0.4  0.32 0.28]\n",
      "Happy [0.4  0.31 0.29]\n",
      "Happy [0.55 0.19 0.26]\n",
      "Happy [0.75 0.07 0.18]\n",
      "Happy [0.87 0.01 0.12]\n",
      "Happy [0.89 0.02 0.09]\n",
      "Happy [0.87 0.03 0.1 ]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.78 0.12 0.1 ]\n",
      "Happy [0.77 0.13 0.1 ]\n",
      "Happy [0.76 0.13 0.11]\n",
      "Happy [0.72 0.14 0.14]\n",
      "Happy [0.75 0.15 0.1 ]\n",
      "Happy [0.76 0.14 0.1 ]\n",
      "Happy [0.77 0.14 0.09]\n",
      "Happy [0.75 0.14 0.11]\n",
      "Happy [0.75 0.12 0.13]\n",
      "Happy [0.71 0.14 0.15]\n",
      "Sad [0.21 0.68 0.11]\n",
      "Sad [0.13 0.75 0.12]\n",
      "Sad [0.12 0.78 0.1 ]\n",
      "Sad [0.1  0.76 0.14]\n",
      "Sad [0.06 0.77 0.17]\n",
      "Sad [0.06 0.78 0.16]\n",
      "Sad [0.06 0.81 0.13]\n",
      "Sad [0.05 0.83 0.12]\n",
      "Sad [0.05 0.81 0.14]\n",
      "Sad [0.05 0.79 0.16]\n",
      "Sad [0.04 0.8  0.16]\n",
      "Sad [0.03 0.79 0.18]\n",
      "Sad [0.03 0.79 0.18]\n",
      "Sad [0.03 0.77 0.2 ]\n",
      "Sad [0.03 0.77 0.2 ]\n",
      "Sad [0.03 0.77 0.2 ]\n",
      "Sad [0.03 0.77 0.2 ]\n",
      "Sad [0.02 0.8  0.18]\n",
      "Sad [0.03 0.85 0.12]\n",
      "Sad [0.03 0.84 0.13]\n",
      "Sad [0.02 0.87 0.11]\n",
      "Sad [0.03 0.86 0.11]\n",
      "Sad [0.01 0.92 0.07]\n",
      "Sad [0.01 0.9  0.09]\n",
      "Sad [0.02 0.89 0.09]\n",
      "Sad [0.02 0.9  0.08]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.01 0.96 0.03]\n",
      "Sad [0.01 0.95 0.04]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.02 0.92 0.06]\n",
      "Sad [0.02 0.93 0.05]\n",
      "Sad [0.02 0.92 0.06]\n",
      "Sad [0.02 0.91 0.07]\n",
      "Sad [0.02 0.9  0.08]\n",
      "Sad [0.02 0.9  0.08]\n",
      "Sad [0.01 0.9  0.09]\n",
      "Sad [0.02 0.9  0.08]\n",
      "Sad [0.02 0.87 0.11]\n",
      "Sad [0.02 0.87 0.11]\n",
      "Sad [0.03 0.84 0.13]\n",
      "Sad [0.03 0.82 0.15]\n",
      "Sad [0.03 0.83 0.14]\n",
      "Sad [0.03 0.83 0.14]\n",
      "Sad [0.02 0.87 0.11]\n",
      "Sad [0.03 0.89 0.08]\n",
      "Sad [0.03 0.91 0.06]\n",
      "Sad [0.03 0.92 0.05]\n",
      "Sad [0.09 0.83 0.08]\n",
      "Sad [0.09 0.83 0.08]\n",
      "Sad [0.35 0.51 0.14]\n",
      "Happy [0.61 0.29 0.1 ]\n",
      "Happy [0.75 0.21 0.04]\n",
      "Happy [0.81 0.15 0.04]\n",
      "Happy [0.8 0.1 0.1]\n",
      "Happy [0.86 0.07 0.07]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.89 0.06 0.05]\n",
      "Happy [0.87 0.05 0.08]\n",
      "Happy [0.85 0.07 0.08]\n",
      "Happy [0.82 0.09 0.09]\n",
      "Happy [0.69 0.16 0.15]\n",
      "Happy [0.63 0.19 0.18]\n",
      "Happy [0.47 0.33 0.2 ]\n",
      "Happy [0.45 0.33 0.22]\n",
      "Happy [0.42 0.33 0.25]\n",
      "Happy [0.44 0.3  0.26]\n",
      "Happy [0.44 0.3  0.26]\n",
      "Happy [0.41 0.32 0.27]\n",
      "Happy [0.36 0.33 0.31]\n",
      "Happy [0.38 0.32 0.3 ]\n",
      "Happy [0.36 0.32 0.32]\n",
      "Happy [0.37 0.32 0.31]\n",
      "Happy [0.37 0.33 0.3 ]\n",
      "Happy [0.36 0.34 0.3 ]\n",
      "Happy [0.35 0.32 0.33]\n",
      "Happy [0.35 0.32 0.33]\n",
      "Happy [0.35 0.3  0.35]\n",
      "Happy [0.36 0.32 0.32]\n",
      "Happy [0.36 0.32 0.32]\n",
      "Happy [0.35 0.33 0.32]\n",
      "Happy [0.34 0.33 0.33]\n",
      "Happy [0.34 0.32 0.34]\n",
      "Happy [0.36 0.33 0.31]\n",
      "Happy [0.35 0.34 0.31]\n",
      "Happy [0.35 0.34 0.31]\n",
      "Happy [0.36 0.34 0.3 ]\n",
      "Sad [0.35 0.38 0.27]\n",
      "Sad [0.35 0.37 0.28]\n",
      "Sad [0.35 0.37 0.28]\n",
      "Sad [0.34 0.38 0.28]\n",
      "Sad [0.35 0.37 0.28]\n",
      "Happy [0.36 0.36 0.28]\n",
      "Sad [0.33 0.4  0.27]\n",
      "Sad [0.35 0.38 0.27]\n",
      "Sad [0.32 0.38 0.3 ]\n",
      "Sad [0.32 0.35 0.33]\n",
      "Sad [0.36 0.38 0.26]\n",
      "Sad [0.3  0.39 0.31]\n",
      "Sad [0.3  0.38 0.32]\n",
      "Sad [0.3  0.39 0.31]\n",
      "Sad [0.29 0.37 0.34]\n",
      "Sad [0.31 0.36 0.33]\n",
      "Sad [0.3  0.37 0.33]\n",
      "Sad [0.31 0.35 0.34]\n",
      "Victorius [0.25 0.35 0.4 ]\n",
      "Victorius [0.27 0.35 0.38]\n",
      "Victorius [0.27 0.35 0.38]\n",
      "Victorius [0.27 0.34 0.39]\n",
      "Victorius [0.22 0.34 0.44]\n",
      "Victorius [0.22 0.34 0.44]\n",
      "Victorius [0.23 0.34 0.43]\n",
      "Victorius [0.25 0.31 0.44]\n",
      "Victorius [0.27 0.35 0.38]\n",
      "Happy [0.71 0.07 0.22]\n",
      "Happy [0.7  0.09 0.21]\n",
      "Happy [0.73 0.07 0.2 ]\n",
      "Happy [0.73 0.06 0.21]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor feed\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Make detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        image.flags.writeable=True\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=1,circle_radius=1))\n",
    "\n",
    "        # 2. Left hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 3. Right hand landmarks\n",
    "        mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "\n",
    "        # 4. Pose detection\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(240,0,0),thickness=2,circle_radius=2))\n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x,landmark.y,landmark.z,landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # Extract face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x,landmark.y,landmark.z,landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            # # Concat rows\n",
    "            row = pose_row+face_row\n",
    "\n",
    "            # # Insert row\n",
    "            # row.insert(0,class_name)\n",
    "\n",
    "            # # Export to csv\n",
    "            # with open('coords.csv',mode='a',newline='') as f:\n",
    "            #     csv_writer = csv.writer(f,delimiter=\",\",quotechar=\"'\",quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row)\n",
    "\n",
    "            # Make detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_pred = model.predict_proba(X)[0]\n",
    "            print(body_language_class,body_language_pred)\n",
    "\n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(np.array((results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x,\n",
    "                                                results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y)\n",
    "                                                ),[640,480]).astype(int))\n",
    "            \n",
    "            cv2.rectangle(image,\n",
    "                         (coords[0],coords[1]+5),\n",
    "                         (coords[0]+len(body_language_class)*20,coords[1]-30),\n",
    "                         (245,117,16),-1)\n",
    "            cv2.putText(image,body_language_class,coords,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "            \n",
    "            # Display class\n",
    "            cv2.putText(image,\"CLASS\",\n",
    "                        (95,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "            cv2.putText(image,body_language_class.split(\" \")[0],\n",
    "                        (90,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "            \n",
    "            # Display prob\n",
    "            cv2.putText(image,\"PROB\",\n",
    "                        (15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "            cv2.putText(image,str(round(body_language_pred[np.argmax(body_language_pred)],2)),\n",
    "                        (10,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        cv2.imshow(\"Holistic model detection\",image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5581431984901428"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
